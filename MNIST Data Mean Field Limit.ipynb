{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad5a5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.sparse import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time # to be used in loop iterations\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38fdd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x, β = 1):\n",
    "    \"\"\"\n",
    "    Computes the softplus function of the given input x.\n",
    "    \n",
    "    Args:\n",
    "        x (float or numpy.ndarray): The input values to compute the softplus function for.\n",
    "        β (float, optional): The scaling factor for the input x. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        float or numpy.ndarray: The output of the softplus function, with the same shape as x.\n",
    "    \"\"\"\n",
    "    return np.log(1+np.exp(-np.abs(x))) + np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2bd8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(h, W, b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the output of a dense layer given an input h, a kernel matrix W, and bias parameters b.\n",
    "\n",
    "    Args:\n",
    "        h (numpy.ndarray): A K x h_in array of input values.\n",
    "        W (numpy.ndarray): An h_in x h_out array for kernel matrix parameters.\n",
    "        b (numpy.ndarray): A length h_out 1-D array for bias parameters.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A K x h_out array containing the output of the dense layer.\n",
    "    \"\"\"\n",
    "    return  h @ W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23823627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_derivative(a):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the activation function used in a neural network's hidden layer.\n",
    "\n",
    "    The activation function used in this implementation is a\\\n",
    "    numerically stable version of the ReLU functiongiven by:\\\n",
    "    1 / (1 + exp(-a)) for a >= 0, and exp(a) / (1 + exp(a)) for a < 0.\n",
    "\n",
    "    Args:\n",
    "        a (numpy.ndarray): A K x 200 array of hidden layer pre-activations.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A K x 200 array of diagonal elements\\\n",
    "        containing the derivative of the activation function with respect\n",
    "        to the input a.\n",
    "    \"\"\"\n",
    "    \n",
    "    derivative = np.where(a >= 0, \n",
    "                1 / (1 + np.exp(-a)), \n",
    "                np.exp(a) / (1 + np.exp(a)))\n",
    "\n",
    "    \n",
    "    return  derivative # dlog(1+exp(x)) = exp(x)/(1+exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54b84d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    \"\"\"\n",
    "    Encodes a 1-D array of labels into one-hot vectors.\n",
    "\n",
    "    Arguments:\n",
    "    y: 1-D numpy array of shape (N,), where N is the number of samples.\n",
    "       The values represent the class labels.\n",
    "\n",
    "    Returns:\n",
    "    A numpy array of shape (N, num_classes), where num_classes is the number of unique labels.\n",
    "    Each row represents the one-hot encoding of the corresponding label in y.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_classes = np.max(y) + 1  # number of unique classes\n",
    "    y_batch_prob = np.zeros((y.shape[0], num_classes))\n",
    "\n",
    "    for i in range(y.shape[0]):\n",
    "        y_batch_prob[i, y[i]] = 1.\n",
    "\n",
    "    return y_batch_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21eb7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural (x, W0, b0, W1, b1, β=1):\n",
    "    \"\"\"\n",
    "    Implements a 1-layer neural network with the given weights and biases,\\\n",
    "    and applies a softmax function to the output.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): A K x N array of input data.\n",
    "        W0 (numpy.ndarray): An h_in x h_out array of weights for the first hidden layer.\n",
    "        b0 (numpy.ndarray): A length h_out 1-D array of biases for the first hidden layer.\n",
    "        W1 (numpy.ndarray): An h_out x h_out array of weights for the second hidden layer.\n",
    "        b1 (numpy.ndarray): A length h_out 1-D array of biases for the second hidden layer.\n",
    "        β (float, optional): The β value to use for the softplus activation function. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A K x 10 output array containing the softmax activation of the neural network's output layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the activations and outputs for the input layer and each hidden layer\n",
    "    a1 = dense(x, W0, b0)\n",
    "    h1 = softplus(a1, β)\n",
    "    \n",
    "    # Compute the pre-softmax output of the neural network\n",
    "    a2 = dense(h1, W1, b1)\n",
    "    \n",
    "    # Apply the softmax function to the output of the neural network\n",
    "    return softmax(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c24af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss(x_batch, y_batch, W0, b0, W1, b1):\n",
    "    \"\"\"\n",
    "    Computes the Kullback-Leibler divergence loss between predicted and actual labels.\n",
    "\n",
    "    Arguments:\n",
    "    x_batch: 2-D numpy array of shape (K, D), where K is the batch size and D is the number of features.\n",
    "             The input data for the batch.\n",
    "    y_batch: 1-D numpy array of shape (K,), where K is the batch size.\n",
    "             The actual class labels for the input data.\n",
    "    W0: 2-D numpy array of shape (D, h), where h is the number of hidden units in the first layer.\n",
    "    b0: 1-D numpy array of shape (h,), where h is the number of hidden units in the first layer.\n",
    "        The bias parameters for the first layer.\n",
    "    W1: 2-D numpy array of shape (h, h), where h is the number of hidden units in each hidden layer.\n",
    "    b1: 1-D numpy array of shape (h,), where h is the number of hidden units in each hidden layer.\n",
    "        The bias parameters for each hidden layer.\n",
    "    W2: 2-D numpy array of shape (h, h), where h is the number of hidden units in each hidden layer.\n",
    "    b2: 1-D numpy array of shape (h,), where h is the number of hidden units in each hidden layer.\n",
    "        The bias parameters for each hidden layer.\n",
    "    W3: 2-D numpy array of shape (h, num_classes), where num_classes is the number of unique labels.\n",
    "    b3: 1-D numpy array of shape (num_classes,), where num_classes is the number of unique labels.\n",
    "        The bias parameters for the output layer.\n",
    "\n",
    "    Returns:\n",
    "    The mean Kullback-Leibler divergence loss over the batch.\n",
    "    \"\"\"\n",
    "    y_out = mlp(x_batch, W0, b0, W1, b1)\n",
    "    \n",
    "    y_batch = one_hot_encode(y_batch)\n",
    "\n",
    "    #modification to KL divergence to ensure numerical stability\n",
    "    KL_divergence = y_batch[np.where(y_batch != 0)]*np.log(y_batch[np.where(y_batch != 0)]/y_out[np.where(y_batch != 0)])\n",
    "    \n",
    "    return np.mean(KL_divergence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84838c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(x_batch, y_batch, W0, b0, W1,b1, β = 1):\n",
    "    \"\"\"\n",
    "    Computes the gradients of the weights and biases in the neural network\\\n",
    "    for a given batch of inputs and labels.\n",
    "    \n",
    "    Args:\n",
    "        x_batch (np.ndarray): K x D array of inputs\n",
    "        y_batch (np.ndarray): K x 1 array of outputs\n",
    "        W0 (np.ndarray): D x 200 array of weights for the first hidden layer\n",
    "        b0 (np.ndarray): 1 x 200 array of biases for the first hidden layer\n",
    "        W1 (np.ndarray): 200 x 200 array of weights for the second hidden layer\n",
    "        b1 (np.ndarray): 1 x 200 array of biases for the second hidden layer\n",
    "        W2 (np.ndarray): 200 x 200 array of weights for the third hidden layer\n",
    "        b2 (np.ndarray): 1 x 200 array of biases for the third hidden layer\n",
    "        W3 (np.ndarray): 200 x 10 array of weights for the output layer\n",
    "        b3 (np.ndarray): 1 x 10 array of biases for the output layer\n",
    "        β (float): Scaling factor for the softplus activation function. Default is 1.\n",
    "\n",
    "    Returns:\n",
    "        dW0 (np.ndarray): D x 200 array of weight gradients for the first hidden layer\n",
    "        db0 (np.ndarray): 1 x 200 array of bias gradients for the first hidden layer\n",
    "        dW1 (np.ndarray): 200 x 200 array of weight gradients for the second hidden layer\n",
    "        db1 (np.ndarray): 1 x 200 array of bias gradients for the second hidden layer\n",
    "        dW2 (np.ndarray): 200 x 200 array of weight gradients for the third hidden layer\n",
    "        db2 (np.ndarray): 1 x 200 array of bias gradients for the third hidden layer\n",
    "        dW3 (np.ndarray): 200 x 10 array of weight gradients for the output layer\n",
    "        db3 (np.ndarray): 1 x 10 array of bias gradients for the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    #propagate forward x_batch through network:\n",
    "        \n",
    "    a1 = dense(x_batch, W0, b0)\n",
    "    h1 = softplus(a1, β)\n",
    "    \n",
    "\n",
    "    #pre-softmax output\n",
    "\n",
    "    a2 = dense(h1, W1, b1)\n",
    "\n",
    "    #Backward pass\n",
    "    \n",
    "    #one-hot encoding of labels\n",
    "    y_batch_prob = one_hot_encode(y_batch)\n",
    "        \n",
    "    \n",
    "    delta2 = softmax(a2) - y_batch_prob  \n",
    "    delta1 =  activation_derivative(a1)*(delta2@W1.T) \n",
    "    \n",
    "    # gradients\n",
    "\n",
    "    dW1 = (1/x_batch.shape[0])*(delta2.T@h1).T\n",
    "    db1 = np.mean(delta2, axis = 0)\n",
    "    \n",
    "    \n",
    "    dW0 = (1/x_batch.shape[0])*(delta1.T@x_batch).T\n",
    "    db0 = np.mean(delta1, axis = 0)\n",
    "\n",
    "    return dW0, db0, dW1,db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42d15ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the SGD algorithm\n",
    "def train_mlp(X_train, y_train, X_test, y_test, P = 200, learning_rate = 0.01, batch_size = 128, epochs = 40, scores = False):\n",
    "    \"\"\"\n",
    "    The train_mlp function trains a multi-layer perceptron (MLP) on a given training dataset and returns the trained weights and biases. The function also computes the KL divergence loss and accuracy scores for both the training and test datasets over the training epochs.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        X_train: 2D numpy array of shape (n_samples, n_features) containing the training data features.\n",
    "        y_train: 1D numpy array of shape (n_samples,) containing the training data labels.\n",
    "        X_test: 2D numpy array of shape (n_samples, n_features) containing the test data features.\n",
    "        y_test: 1D numpy array of shape (n_samples,) containing the test data labels.\n",
    "        learning_rate: float, optional learning rate for the optimizer, default is 0.01.\n",
    "        batch_size: int, optional mini-batch size for the stochastic gradient descent, default is 128.\n",
    "        epochs: int, optional number of training epochs, default is 40.\n",
    "        scores: boolean, optional flag to return loss and accuracy scores, default is False.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        If scores is False, returns the final trained weights and biases for each layer and test loss during training:\n",
    "            W0: 2D numpy array of shape (n_features, 200) representing the weights for the first hidden layer.\n",
    "            b0: 2D numpy array of shape (1, 200) representing the biases for the first hidden layer.\n",
    "            W1: 2D numpy array of shape (200, 200) representing the weights for the second hidden layer.\n",
    "            b1: 2D numpy array of shape (1, 200) representing the biases for the second hidden layer.\n",
    "            losses_test: numpy array of shape (epochs,) containing test losses during training.\n",
    "            \n",
    "        If scores is True, returns the final trained weights and biases for each layer along\\\n",
    "        with the loss and accuracy scores for both the training and test datasets:\n",
    "            losses_train: 1D numpy array of shape (epochs,) \\\n",
    "            containing the KL divergence loss for each epoch on the training dataset.\n",
    "            losses_test: 1D numpy array of shape (epochs,)\\\n",
    "            containing the KL divergence loss for each epoch on the test dataset.\n",
    "            accuracy_train: 1D numpy array of shape (epochs,)\\\n",
    "            containing the mean accuracy score for each epoch on the training dataset.\n",
    "            accuracy_test: 1D numpy array of shape (epochs,)\\\n",
    "            containing the mean accuracy score for each epoch on the test dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialise weights\n",
    "    \n",
    "    N,d = X_train.shape\n",
    "    \n",
    "    var0 = 6. / (d+P)\n",
    "    W0 =  np.random.uniform(-np.sqrt(var0), np.sqrt(var0), (d, P) )\n",
    "    b0 = np.zeros(P).reshape(1,-1)\n",
    "    \n",
    "    var1 = 6. / (P+10)\n",
    "    W1 =  np.random.uniform(-np.sqrt(var1), np.sqrt(var1), (P, 10) )\n",
    "    b1 = np.zeros(10).reshape(1,-1)\n",
    "    \n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    \n",
    "    for epoch in trange(epochs):\n",
    "        #shuffle data\n",
    "        p = np.random.permutation(N)\n",
    "        X_train = X_train[p]\n",
    "        y_train = y_train[p]\n",
    "        \n",
    "        # iterate over minibatches\n",
    "        indices = np.arange(N)\n",
    "\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for start_idx in range(0, N - batch_size +1, batch_size):\n",
    "            \n",
    "            batch_idx = indices[start_idx:start_idx + batch_size]\n",
    "            \n",
    "            #sampling minibatch\n",
    "            x_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "            \n",
    "            dW0, db0, dW1,db1 =\\\n",
    "            backpropagate(x_batch, y_batch, W0, b0, W1,b1)\n",
    "            \n",
    "            W0 -= learning_rate * dW0 \n",
    "            b0 -= learning_rate * db0 \n",
    "            W1 -= learning_rate * dW1\n",
    "            b1 -= learning_rate * db1\n",
    "            \n",
    "            \n",
    "        #compute KL divergence loss per epoch\n",
    "        losses_train.append(np.mean(KL_loss(X_train, y_train, W0, b0, W1,b1)))\n",
    "        losses_test.append(np.mean(KL_loss(X_test, y_test, W0, b0, W1,b1)))\n",
    "\n",
    "        #compute mlp predictions\n",
    "        \n",
    "        y_pred_train = np.argmax(mlp(X_train, W0, b0, W1,b1), axis = 1)\n",
    "        \n",
    "        y_pred_test = np.argmax(mlp(X_test, W0, b0, W1,b1), axis = 1)\n",
    "        \n",
    "        #compute model mean accuracy scores\n",
    "        #print(f'Training set accuracy: {len(y_train[y_pred_train == y_train])/len(y_train)}\\n')\n",
    "        #print(f'Training set loss: {loss_train}\\n')\n",
    "        accuracy_train.append(len(y_train[y_pred_train == y_train])/len(y_train))\n",
    "        accuracy_test.append(len(y_test[y_pred_test == y_test])/len(y_test))\n",
    "        \n",
    "    if scores == True:\n",
    "        return losses_train, losses_test, accuracy_train, accuracy_test\n",
    "        \n",
    "    return W0, b0, W1,b1, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab05908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardising dataset\n",
    "def standardise_mlp(X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "\n",
    "    X: numpy array of shape (n_samples, n_features) representing the input data\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    Xbar: numpy array of shape (n_samples, n_features) representing the standardized input data\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std_filled = std.copy()\n",
    "    std_filled[std==0] = 1.\n",
    "    Xstd = ((X-mu)/std_filled)\n",
    "    return Xstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a67a4496",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNIST_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#training set ground truth labels\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      9\u001b[0m train_data \u001b[38;5;241m=\u001b[39m data[data\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#standardise train data\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Load training data \n",
    "\n",
    "#train data\n",
    "data = pd.read_csv('MNIST_train.csv')\n",
    "\n",
    "#training set ground truth labels\n",
    "y_train = np.squeeze(np.array(data['label']))\n",
    "\n",
    "train_data = data[data.columns[1:]].to_numpy()\n",
    "\n",
    "#standardise train data\n",
    "train_data_std = standardise_mlp(train_data)\n",
    "\n",
    "#test data\n",
    "test_data = pd.read_csv('MNIST_test.csv')\n",
    "\n",
    "#test set ground truth labels\n",
    "y_test = np.array(test_data['label'])\n",
    "\n",
    "test_data = test_data[test_data.columns[1:]].to_numpy()\n",
    "\n",
    "#standardise test data\n",
    "test_data_std = standardise_mlp(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268724df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_func(P):\n",
    "    learning_rate = 2.15443469e-02\n",
    "    W0, b0, W1, b1, losses_test = train_mlp(train_data_std, y_train, test_data_std,  y_test, P = P, learning_rate =\\\n",
    "          learning_rate, batch_size = 128, epochs = 30, scores = False)\n",
    "    params = np.ndarray.flatten(W1)\n",
    "    '''np.concatenate((np.ndarray.flatten(W0),np.ndarray.flatten(W1), np.ndarray.flatten(b0),\\\n",
    "                             np.ndarray.flatten(b1)))'''\n",
    "    \n",
    "    return params, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c400047",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m params_2, losses_2 \u001b[38;5;241m=\u001b[39m \u001b[43mparams_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m params_3, losses_2 \u001b[38;5;241m=\u001b[39m params_func(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      3\u001b[0m params_4, losses_2 \u001b[38;5;241m=\u001b[39m params_func(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1e4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mparams_func\u001b[0;34m(P)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparams_func\u001b[39m(P):\n\u001b[1;32m      2\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.15443469e-02\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     W0, b0, W1, b1, losses_test \u001b[38;5;241m=\u001b[39m train_mlp(\u001b[43mtrain_data_std\u001b[49m, y_train, test_data_std,  y_test, P \u001b[38;5;241m=\u001b[39m P, learning_rate \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m      4\u001b[0m           learning_rate, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray\u001b[38;5;241m.\u001b[39mflatten(W1)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''np.concatenate((np.ndarray.flatten(W0),np.ndarray.flatten(W1), np.ndarray.flatten(b0),\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m                             np.ndarray.flatten(b1)))'''\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_std' is not defined"
     ]
    }
   ],
   "source": [
    "params_2, losses_2 = params_func(100)\n",
    "params_3, losses_2 = params_func(1000)\n",
    "params_4, losses_2 = params_func(int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a31d7fd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mparams_2\u001b[49m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP: 1e2\u001b[39m\u001b[38;5;124m'\u001b[39m, bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(params_3, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP: 1e3\u001b[39m\u001b[38;5;124m'\u001b[39m, bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(params_4, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP: 1e4\u001b[39m\u001b[38;5;124m'\u001b[39m, bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_2' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(params_2, label = 'P: 1e2', bins = 100, density = True)\n",
    "plt.hist(params_3, label = 'P: 1e3', bins = 100, density = True)\n",
    "plt.hist(params_4, label = 'P: 1e4', bins = 100, density = True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
